{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Input: the policy $\\pi$ <br>\n",
    "Initialize $Q(s,a)$ arbitrarily <br>\n",
    "Repeat (for each episode): <br>\n",
    "&emsp;    Initialize s <br>\n",
    "&emsp;    Repeat (for each step of episode): <br>\n",
    "&emsp;&emsp;        A <-- action given by $\\pi$ for s <br>\n",
    "&emsp;&emsp;        Taken action A; observe reward, R, and next state, S' <br>\n",
    "&emsp;&emsp;        $Q(s,a)$ <-- $Q(s,a) + \\alpha [R + \\gamma  amax(Q(s',:)) - Q(s,a)]$ <br>\n",
    "&emsp;&emsp;        S <-- S' <br>\n",
    "&emsp;    until S is terminal <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation for Q-Learning using a NN\n",
    "For big state-action-spaces the required memory for Q can exceeed the available RAM.\n",
    "In such cases the lookup table for Q can be approximated by some function e.g. a NN which maps the current state to a corresponding Q-value.\n",
    "With a NN the values for Q can't just simply be updated, instead the algorithm has to figure out how to update the weights of the NN. This is done by using backpropagation using the loss sum(q_target - Q)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysgen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:  0.0\n",
      "avg reward:  0.0196078431372549\n",
      "avg reward:  0.01\n",
      "avg reward:  0.06\n",
      "avg reward:  0.09\n",
      "avg reward:  0.12\n",
      "avg reward:  0.31\n",
      "avg reward:  0.51\n",
      "avg reward:  0.64\n",
      "avg reward:  0.65\n",
      "avg reward:  0.61\n",
      "avg reward:  0.66\n",
      "avg reward:  0.74\n",
      "avg reward:  0.77\n",
      "avg reward:  0.71\n",
      "avg reward:  0.66\n",
      "avg reward:  0.62\n",
      "avg reward:  0.67\n",
      "avg reward:  0.75\n",
      "avg reward:  0.74\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "np.random.seed(42)\n",
    "\n",
    "#env_name = 'Taxi-v2'\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1000 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs1 = tf.placeholder(shape=[1,state_space], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([state_space, action_space], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,action_space], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        epsilon *= epsilon_decay\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while(not done):\n",
    "            action, target_Q = sess.run([predict, Qout], feed_dict={inputs1: np.identity(state_space)[state:state+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action[0] = env.action_space.sample()\n",
    "            state_next, reward, done, _ = env.step(action[0])\n",
    "            episode_reward += reward\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(state_space)[state_next:state_next+1]})\n",
    "            \n",
    "            target_Q[0, action[0]] = reward + gamma*np.max(Q1)\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(state_space)[state:state+1], nextQ:target_Q})\n",
    "            state = state_next\n",
    "            \n",
    "            \n",
    "            if(done):\n",
    "                reward_list.append(episode_reward)\n",
    "                if(game%50 == 0):\n",
    "                    print('avg reward: ', np.mean(reward_list))\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation for Q-Learning using NN\n",
    "For some reason Keras behaves differently than Tensorflow even though Tensorflow is used as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 avg reward:  0.0\n",
      "episode:  200 avg reward:  0.0\n",
      "episode:  400 avg reward:  0.01\n",
      "episode:  600 avg reward:  0.0\n",
      "episode:  800 avg reward:  0.02\n",
      "episode:  1000 avg reward:  0.0\n",
      "episode:  1200 avg reward:  0.02\n",
      "episode:  1400 avg reward:  0.05\n",
      "episode:  1600 avg reward:  0.06\n",
      "episode:  1800 avg reward:  0.07\n",
      "episode:  2000 avg reward:  0.05\n",
      "episode:  2200 avg reward:  0.06\n",
      "episode:  2400 avg reward:  0.04\n",
      "episode:  2600 avg reward:  0.05\n",
      "episode:  2800 avg reward:  0.04\n",
      "episode:  3000 avg reward:  0.03\n",
      "episode:  3200 avg reward:  0.01\n",
      "episode:  3400 avg reward:  0.01\n",
      "episode:  3600 avg reward:  0.02\n",
      "episode:  3800 avg reward:  0.04\n",
      "episode:  4000 avg reward:  0.02\n",
      "episode:  4200 avg reward:  0.01\n",
      "episode:  4400 avg reward:  0.06\n",
      "episode:  4600 avg reward:  0.1\n",
      "episode:  4800 avg reward:  0.02\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "np.random.seed(42)\n",
    "\n",
    "#env_name = 'Taxi-v2'\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.999 # exploration decay\n",
    "num_games = 5000 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        episode_reward += reward\n",
    "        Q1 = state_to_Qvalue(state_next, model)\n",
    "\n",
    "        target_Q[0, action] = reward + gamma*np.max(Q1)\n",
    "        model.train_on_batch(state, target_Q)\n",
    "        #model.fit(state, target_Q, verbose=0, epochs=1)\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%200 == 0):\n",
    "                print('episode: ', game, 'avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a frozen target policy\n",
    "In most cases training with Gradient descent instead of the simple tabular update rule, is more unstable and therefore less efficient. Two extensions to improve the efficiency are Experience Replay and Freezing target Networks. In the approach above where the same networks influence the target and the Q-prediction, an effect similar to a dog hunting its tail can occur. \n",
    "The weights of the network which decides the next action get updated to better fit the target but at the same time the target also moves. To avoid this problem, a seperate target network has fixed weights which only get updated every once in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 avg reward:  0.0\n",
      "episode:  200 avg reward:  0.02\n",
      "episode:  400 avg reward:  0.04\n",
      "episode:  600 avg reward:  0.01\n",
      "episode:  800 avg reward:  0.01\n",
      "episode:  1000 avg reward:  0.04\n",
      "episode:  1200 avg reward:  0.01\n",
      "episode:  1400 avg reward:  0.02\n",
      "episode:  1600 avg reward:  0.07\n",
      "episode:  1800 avg reward:  0.01\n",
      "episode:  2000 avg reward:  0.03\n",
      "episode:  2200 avg reward:  0.03\n",
      "episode:  2400 avg reward:  0.05\n",
      "episode:  2600 avg reward:  0.05\n",
      "episode:  2800 avg reward:  0.02\n",
      "episode:  3000 avg reward:  0.09\n",
      "episode:  3200 avg reward:  0.09\n",
      "episode:  3400 avg reward:  0.06\n",
      "episode:  3600 avg reward:  0.08\n",
      "episode:  3800 avg reward:  0.04\n",
      "episode:  4000 avg reward:  0.09\n",
      "episode:  4200 avg reward:  0.01\n",
      "episode:  4400 avg reward:  0.0\n",
      "episode:  4600 avg reward:  0.01\n",
      "episode:  4800 avg reward:  0.02\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "np.random.seed(42)\n",
    "\n",
    "#env_name = 'Taxi-v2'\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.999 # exploration decay\n",
    "num_games = 5000 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "def target_model_update():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        episode_reward += reward\n",
    "        Q1 = state_to_Qvalue(state_next, target_model)\n",
    "\n",
    "        target_Q[0, action] = reward + gamma*np.max(Q1)\n",
    "        #model.fit(state, target_Q, verbose=0, epochs=1)\n",
    "        model.train_on_batch(state, target_Q)\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            target_model_update()\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%200 == 0):\n",
    "                print('episode: ', game, 'avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding memory replay\n",
    "It is very inefficient to use a played sample only once, memory replay saves a certain number of (s, a, r, s') in a list and reuses samples of this memory for batch training. This can break temporal correlations and speed up training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:  0.0\n",
      "avg reward:  0.0196078431372549\n",
      "avg reward:  0.05\n",
      "avg reward:  0.11\n",
      "avg reward:  0.13\n",
      "avg reward:  0.25\n",
      "avg reward:  0.37\n",
      "avg reward:  0.4\n",
      "avg reward:  0.38\n",
      "avg reward:  0.49\n",
      "avg reward:  0.61\n",
      "avg reward:  0.58\n",
      "avg reward:  0.56\n",
      "avg reward:  0.53\n",
      "avg reward:  0.55\n",
      "avg reward:  0.53\n",
      "avg reward:  0.57\n",
      "avg reward:  0.64\n",
      "avg reward:  0.66\n",
      "avg reward:  0.64\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "import random\n",
    "import time\n",
    "np.random.seed(42)\n",
    "\n",
    "#env_name = 'Taxi-v2'\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1000 \n",
    "memory = deque(maxlen=100)\n",
    "minibatch_size = 20\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "def replay():\n",
    "    minibatch = random.sample(memory, minibatch_size)\n",
    "    minibatch = np.array(minibatch)\n",
    "    not_done_indices = np.where(minibatch[:, 4] == False)\n",
    "    \n",
    "    state = np.vstack(minibatch[:, 0])\n",
    "    action = np.vstack(minibatch[:, 1])\n",
    "\n",
    "    y = np.copy(minibatch[:, 2])\n",
    "    \n",
    "    # If minibatch contains any non-terminal states, use separate update rule for those states\n",
    "    if len(not_done_indices[0]) > 0:\n",
    "        predict_sprime = model.predict(np.vstack(minibatch[:, 3]))\n",
    "        predict_sprime_target = target_model.predict(np.vstack(minibatch[:, 3]))\n",
    "\n",
    "        y[not_done_indices] += np.multiply(gamma, \\\n",
    "                predict_sprime_target[not_done_indices, \\\n",
    "                np.argmax(predict_sprime[not_done_indices, :][0], axis=1)][0])\n",
    "\n",
    "    target = model.predict(state)\n",
    "    actions = np.array(minibatch[:, 1], dtype=int)\n",
    "    target[range(minibatch_size), actions] = y\n",
    "    model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "def target_model_update():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def add_memory(s, a, r, s_prime, done):\n",
    "    memory.append((s, a, r, s_prime, done))\n",
    "    \n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        add_memory(state, action, reward, state_next, done)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if len(memory) > minibatch_size:\n",
    "            replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if(done):\n",
    "            target_model_update()\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%50 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027473787777125835"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_102_input to have shape (24,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f26367e48733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtarget_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_to_Qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-f26367e48733>\u001b[0m in \u001b[0;36mstate_to_Qvalue\u001b[0;34m(state, model)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstate_to_Qvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mQvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_102_input to have shape (24,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "import random\n",
    "import time\n",
    "np.random.seed(42)\n",
    "\n",
    "#env_name = 'Taxi-v2'\n",
    "env_name = 'BipedalWalker-v2'\n",
    "episode_length=2000\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1000 \n",
    "memory = deque(maxlen=100)\n",
    "minibatch_size = 20\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "def replay():\n",
    "    minibatch = random.sample(memory, minibatch_size)\n",
    "    minibatch = np.array(minibatch)\n",
    "    not_done_indices = np.where(minibatch[:, 4] == False)\n",
    "    \n",
    "    state = np.vstack(minibatch[:, 0])\n",
    "    action = np.vstack(minibatch[:, 1])\n",
    "\n",
    "    y = np.copy(minibatch[:, 2])\n",
    "    \n",
    "    # If minibatch contains any non-terminal states, use separate update rule for those states\n",
    "    if len(not_done_indices[0]) > 0:\n",
    "        predict_sprime = model.predict(np.vstack(minibatch[:, 3]))\n",
    "        predict_sprime_target = target_model.predict(np.vstack(minibatch[:, 3]))\n",
    "\n",
    "        y[not_done_indices] += np.multiply(gamma, \\\n",
    "                predict_sprime_target[not_done_indices, \\\n",
    "                np.argmax(predict_sprime[not_done_indices, :][0], axis=1)][0])\n",
    "\n",
    "    target = model.predict(state)\n",
    "    actions = np.array(minibatch[:, 1], dtype=int)\n",
    "    target[range(minibatch_size), actions] = y\n",
    "    model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "def target_model_update():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def add_memory(s, a, r, s_prime, done):\n",
    "    memory.append((s, a, r, s_prime, done))\n",
    "    \n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    #state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    for i in range(episode_length):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        add_memory(state, action, reward, state_next, done)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if len(memory) > minibatch_size:\n",
    "            replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if(done):\n",
    "            target_model_update()\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%50 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
