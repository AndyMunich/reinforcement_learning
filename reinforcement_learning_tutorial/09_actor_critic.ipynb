{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradients have the disadvantage of Monte Carlo algorithms. A full episode is sampled, if the episode went well, all actions get considered as good even if there where some bad decisions on the way. In Actor Critics we make an update at each step using a NN to predict the rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient update\n",
    "\\begin{equation*}\n",
    "\\Delta \\theta = \\alpha * \\nabla_\\theta (log(\\pi(a_t|s_t,\\theta)))*R(t)\n",
    "\\end{equation*}\n",
    "\n",
    "Actor Critic update\n",
    "\\begin{equation*}\n",
    "\\Delta \\theta = \\alpha * \\nabla_\\theta (log(\\pi(a_t|s_t,\\theta)))*Q(s_t, a_t)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an actor critic we have two neural nets <br>\n",
    "*** actor: *** $\\pi(a|s, \\theta)$ who decides which actions to take<br>\n",
    "*** critic: *** $Q(a|s, w)$ who approximates $Q(s_t, a_t)$ in the above equation (same idea as in TD) to update the weights of the critic <br>\n",
    "\n",
    "Having two neural nets means that two seperate sets of weights have to be updated: <br>\n",
    "\\begin{equation*}\n",
    "\\Delta \\theta = \\alpha \\nabla_\\theta (log(\\pi_\\theta(s,a)))Q_w(s,a)\n",
    "\\end{equation*}\n",
    "With $ Q_w(s,a) $ being a q learning function approximation <br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w = \\beta (R(s,a) + \\gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t))) \\nabla_w Q(s_t, a_t)\n",
    "\\end{equation*}\n",
    "With $\\beta$ being a seperate learning rate, $(R(s,a) + \\gamma Q_w(s_{t+1}, a_{t+1} - Q_w(s_t, a_t)))$ is the TD-error, $\\nabla_w Q(s_t, a_t)$ is the Gradient of the value function <br>\n",
    "Note: this is the update rule of TD since the argmax is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:\n",
    "- At each time step the current state $s_t$ is fed into the actor --> $a_t$ <br>\n",
    "- $a_t$ is fed into the environment and outputs a reward $r_{t+1}$ and $s_{t+1}$\n",
    "- given $s_t$ and $a_t$ the critic calculates $Q(s_t,a_t)$ with which the actor is updated by $\\Delta \\theta = \\alpha \\nabla_\\theta (log(\\pi_\\theta(s,a)))Q_w(s,a)\n",
    "$\n",
    "- with the updated actor, $a_{t+1}$ is calculated and the critic is updated $\\beta (R(s,a) + \\gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t))) \\nabla_w Q(s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C (Advantage Actor Critic) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "reward_list = deque(maxlen=20)\n",
    "num_games = 300\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.value_size = 1\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.01\n",
    "        self.critic_lr = 0.05\n",
    "\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(8, input_dim=self.state_shape, activation='relu',\n",
    "                        kernel_initializer='he_uniform', use_bias=False))\n",
    "        actor.add(Dense(self.action_shape, activation='softmax',\n",
    "                        kernel_initializer='he_uniform', use_bias=False))\n",
    "        \n",
    "        # Using categorical crossentropy as a loss is a trick to easily\n",
    "        # implement the policy gradient. Categorical cross entropy is defined\n",
    "        # H(p, q) = sum(p_i * log(q_i)). \n",
    "        # p_a = advantage. q_a is the output of the policy network, which is\n",
    "        # the probability of taking the action a, i.e. policy(s, a). \n",
    "        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))\n",
    "        actor.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(8, input_dim=self.state_shape, activation='relu',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        critic.add(Dense(self.value_size, activation='linear',\n",
    "                         kernel_initializer='he_uniform'))\n",
    "        \n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_shape, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_shape))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.train_on_batch(state, advantages)\n",
    "        self.critic.train_on_batch(state, target)\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "agent = A2CAgent(state_shape, action_shape)\n",
    "\n",
    "for game in range(num_games):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_shape])\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        next_state = np.reshape(next_state, [1, state_shape])\n",
    "        agent.train_model(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            reward_list.append(episode_reward)\n",
    "            if( ((game+1)%20) == 0 ):\n",
    "                print(\"episode:\", game+1, \"  score:\", np.mean(reward_list))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
