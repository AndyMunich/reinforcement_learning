{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In contrast to Sarsa which is an on-policy method (it learns to improve the policy while following it), Q-learning is off-policy (it improves Q independent of the policy being followed).\n",
    "The algorithm is nearly identical to Sarsa, with the following difference: <br>\n",
    "$Q(s,a)$ <-- $Q(s,a) + \\alpha [R + \\gamma  Q(s',a') - Q(s,a)]$ <br> \n",
    "$Q(s,a)$ <-- $Q(s,a) + \\alpha [R + \\gamma  amax(Q(s',:)) - Q(s,a)]$ <br> \n",
    "amax(Q(s',:)) chooses the best possible action value available at the next state.\n",
    "In Sutton & Barto, Sarsa performs better an the cliff-walking task but in the \"Taxi-v2\" and \"FrozenLake\" environments Q-Learning outperforms Sarsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Input: the policy $\\pi$ <br>\n",
    "Initialize $Q(s,a)$ arbitrarily <br>\n",
    "Repeat (for each episode): <br>\n",
    "&emsp;    Initialize s <br>\n",
    "&emsp;    Repeat (for each step of episode): <br>\n",
    "&emsp;&emsp;        A <-- action given by $\\pi$ for s <br>\n",
    "&emsp;&emsp;        Taken action A; observe reward, R, and next state, S' <br>\n",
    "&emsp;&emsp;        $Q(s,a)$ <-- $Q(s,a) + \\alpha [R + \\gamma  amax(Q(s',:)) - Q(s,a)]$ <br>\n",
    "&emsp;&emsp;        S <-- S' <br>\n",
    "&emsp;    until S is terminal <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tabular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "alpha = 0.85\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1500 \n",
    "\n",
    "\n",
    "q = np.zeros([state_space, action_space])\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "def choose_action(q, state, epsilon):\n",
    "    ''' epsilon-greedy policy (explore with probability epsilon)  '''\n",
    "    if(np.random.uniform() < epsilon):\n",
    "        action = np.random.choice(action_space) # exploration\n",
    "    else:\n",
    "        action = np.argmax(q[state, :]) # exploitation\n",
    "    return action\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    action = choose_action(q, state, epsilon)\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while(not done): \n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        action_next = choose_action(q, state_next, epsilon)\n",
    "        q[state, action] = q[state, action] + alpha*( reward + gamma*(np.amax(q[state_next, :])) - q[state, action] )\n",
    "        state = state_next\n",
    "        action = action_next\n",
    "        time2 = time.time()\n",
    "        if(done):\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%100 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning using a NN\n",
    "For big state-action-spaces the required memory for Q can exceeed the available RAM.\n",
    "In such cases the lookup table for Q can be approximated by some function e.g. a NN which maps the current state to a corresponding Q-value.\n",
    "With a NN the values for Q can't just simply be updated, instead the algorithm has to figure out how to update the weights of the NN. This is done by using backpropagation using the loss sum(q_target - Q)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1500 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputs1 = tf.placeholder(shape=[1,state_space], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([state_space, action_space], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,action_space], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        epsilon *= epsilon_decay\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while(not done):\n",
    "            action, target_Q = sess.run([predict, Qout], feed_dict={inputs1: np.identity(state_space)[state:state+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action[0] = env.action_space.sample()\n",
    "            state_next, reward, done, _ = env.step(action[0])\n",
    "            episode_reward += reward\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(state_space)[state_next:state_next+1]})\n",
    "            \n",
    "            target_Q[0, action[0]] = reward + gamma*np.max(Q1)\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(state_space)[state:state+1], nextQ:target_Q})\n",
    "            state = state_next\n",
    "            \n",
    "            \n",
    "            if(done):\n",
    "                reward_list.append(episode_reward)\n",
    "                if(game%50 == 0):\n",
    "                    print('avg reward: ', np.mean(reward_list))\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases training with Gradient descent instead of the simple tabular update rule, is more unstable and therefore less efficient.\n",
    "Two extensions to improve the efficiency are Experience Replay and Freezing target Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation for Q-Learning using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:  -821.0\n",
      "avg reward:  -662.6470588235294\n",
      "avg reward:  -559.14\n",
      "avg reward:  -385.13\n",
      "avg reward:  -288.81\n",
      "avg reward:  -242.89\n",
      "avg reward:  -203.66\n",
      "avg reward:  -184.24\n",
      "avg reward:  -168.93\n",
      "avg reward:  -156.68\n",
      "avg reward:  -149.23\n",
      "avg reward:  -139.01\n",
      "avg reward:  -138.23\n",
      "avg reward:  -128.56\n",
      "avg reward:  -125.64\n",
      "avg reward:  -119.51\n",
      "avg reward:  -105.7\n",
      "avg reward:  -99.09\n",
      "avg reward:  -97.78\n",
      "avg reward:  -106.69\n",
      "avg reward:  -86.45\n",
      "avg reward:  -71.27\n",
      "avg reward:  -77.04\n",
      "avg reward:  -70.01\n",
      "avg reward:  -60.96\n",
      "avg reward:  -57.75\n",
      "avg reward:  -52.83\n",
      "avg reward:  -43.54\n",
      "avg reward:  -40.29\n",
      "avg reward:  -37.93\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1500 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "init = RandomUniform(minval=0.0, maxval = 0.01)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    #model.add(Dense(action_space, activation='linear'))\n",
    "    model.add(Dense(action_space, input_dim=state_space, activation='linear', kernel_initializer=init, use_bias=False))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        episode_reward += reward\n",
    "        Q1 = state_to_Qvalue(state_next, model)\n",
    "\n",
    "        target_Q[0, action] = reward + gamma*np.max(Q1)\n",
    "        model.fit(state, target_Q, verbose=0, epochs=1)\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%50 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a frozen target policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:  -884.0\n",
      "avg reward:  -650.5882352941177\n",
      "avg reward:  -558.61\n",
      "avg reward:  -416.74\n",
      "avg reward:  -328.3\n",
      "avg reward:  -272.69\n",
      "avg reward:  -236.33\n",
      "avg reward:  -208.78\n",
      "avg reward:  -179.42\n",
      "avg reward:  -161.08\n",
      "avg reward:  -156.77\n",
      "avg reward:  -154.71\n",
      "avg reward:  -154.42\n",
      "avg reward:  -156.79\n",
      "avg reward:  -160.03\n",
      "avg reward:  -164.95\n",
      "avg reward:  -167.91\n",
      "avg reward:  -167.69\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2de6bf545bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtarget_Q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1500 \n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "init = RandomUniform(minval=0.0, maxval = 0.01)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    #model.add(Dense(action_space, activation='linear'))\n",
    "    model.add(Dense(action_space, input_dim=state_space, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "def target_model_update():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        episode_reward += reward\n",
    "        Q1 = state_to_Qvalue(state_next, target_model)\n",
    "\n",
    "        target_Q[0, action] = reward + gamma*np.max(Q1)\n",
    "        model.fit(state, target_Q, verbose=0, epochs=1)\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            target_model_update()\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%50 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding memory replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:  -866.0\n",
      "avg reward:  -645.9411764705883\n",
      "avg reward:  -547.33\n",
      "avg reward:  -410.26\n",
      "avg reward:  -329.78\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "import random\n",
    "\n",
    "env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "epsilon = 1.0 # amount of exploration\n",
    "epsilon_decay = 0.99 # exploration decay\n",
    "num_games = 1500 \n",
    "memory = deque(maxlen=500)\n",
    "minibatch_size = 30\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "init = RandomUniform(minval=0.0, maxval = 0.01)\n",
    "\n",
    "def replay():\n",
    "    minibatch = random.sample(memory, minibatch_size)\n",
    "    minibatch = np.array(minibatch)\n",
    "    not_done_indices = np.where(minibatch[:, 4] == False)\n",
    "    \n",
    "    state = np.vstack(minibatch[:, 0])\n",
    "    action = np.vstack(minibatch[:, 1])\n",
    "\n",
    "    y = np.copy(minibatch[:, 2])\n",
    "    \n",
    "    # If minibatch contains any non-terminal states, use separate update rule for those states\n",
    "    if len(not_done_indices[0]) > 0:\n",
    "        predict_sprime = model.predict(np.vstack(minibatch[:, 3]))\n",
    "        predict_sprime_target = target_model.predict(np.vstack(minibatch[:, 3]))\n",
    "\n",
    "        y[not_done_indices] += np.multiply(gamma, \\\n",
    "                predict_sprime_target[not_done_indices, \\\n",
    "                np.argmax(predict_sprime[not_done_indices, :][0], axis=1)][0])\n",
    "\n",
    "    target = model.predict(state)\n",
    "    actions = np.array(minibatch[:, 1], dtype=int)\n",
    "    target[range(minibatch_size), actions] = y\n",
    "    model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    #model.add(Dense(action_space, activation='linear'))\n",
    "    model.add(Dense(action_space, input_dim=state_space, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss='mse',optimizer=SGD(lr=0.1))\n",
    "    return model\n",
    "\n",
    "def target_model_update():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def add_memory(s, a, r, s_prime, done):\n",
    "    memory.append((s, a, r, s_prime, done))\n",
    "    \n",
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "def state_to_Qvalue(state, model):\n",
    "    Qvalue = model.predict(state)\n",
    "    return Qvalue\n",
    "\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    epsilon *= epsilon_decay\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done):\n",
    "        target_Q = state_to_Qvalue(state, model)\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(target_Q)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        add_memory(state, action, reward, state_next, done)\n",
    "        episode_reward += reward\n",
    "        Q1 = state_to_Qvalue(state_next, target_model)\n",
    "\n",
    "        target_Q[0, action] = reward + gamma*np.max(Q1)\n",
    "        \n",
    "        if len(memory) > minibatch_size:\n",
    "            replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if(done):\n",
    "            target_model_update()\n",
    "            reward_list.append(episode_reward)\n",
    "            if(game%50 == 0):\n",
    "                print('avg reward: ', np.mean(reward_list))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
