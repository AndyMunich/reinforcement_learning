{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 09. Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value based methods like Q-learning extract the policy out of the value function. But what do you do if the action space is to huge or has continous values? Policy Gradient Methods just like genetic algorithms work directly on the policy and don't take the detour over the value function. <br>\n",
    "<br>\n",
    "__Advantages of policy gradient methods:__ <br>\n",
    "- value based methods can have huge oscilattions while training since very small changes in Q can result in other policies, following the gradient on the other hand should result in smooth policy updates\n",
    "- Policy gradients can handle high dimensional action spaces\n",
    "- Policy gradients can learn stochastic policies which is advantageous in games like stone paper scissors\n",
    "<br>\n",
    "<br>\n",
    "__Disadvantages:__ <br>\n",
    "- getting stuck in local optima\n",
    "- can take more iterations for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning you get labels with each dataset. e.g. images of cats and dogs and the job is to classify them. Lets say the network outputs a probability of [0.6, 0.4] while the label was [0, 1], the standard approach would be to subtract the prediction from the label / feed it in a loss function and backpropagate the error such that the weights get shifted rowards better predictions.\n",
    "\n",
    "Policy gradients are very similar to this approach. Instead of a given Label we got some reward. If we recieved high rewards during the episode, all actions we took are seen as correct decisions.\n",
    "In the case of low rewards we want to change the weights of our policy such that the bad decisions are taken less likely.\n",
    "\n",
    "A simple criterium to seperate good from bad decisions could be to subtract the average reward from our current one. A positive number means we got more than usual, a negative number means the agent performed worse, and the value can be used as a weight, how much we want to change the policy. Lets call this number advantage (adv)\n",
    "\n",
    "For every step of the episode our policy gives us the action probablity:\n",
    "state --> policy --> probability to take an action (prob_a) e.g. [0.6, 0.4] \n",
    "\n",
    "Lets say we got a reward which was better than average and the taken action was [1.0, 0], than we can use this sample just like in supervised learning for backpropagation with model.train_on_batch(state, [1.0*adv, 0]) to change the parameter such that the output gets closer to our desired output.\n",
    "\n",
    "In the case of a low reward (less than average), we want to lower the probability for the chosen action. This is implemented using model.train_on_batch(state, [1.0*adv, 0]) where adv is a negative number.\n",
    "\n",
    "Lets see if this simple approach already works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 avg reward:  0.0\n",
      "episode:  500 avg reward:  0.01\n",
      "episode:  1000 avg reward:  0.02\n",
      "episode:  1500 avg reward:  0.02\n",
      "episode:  2000 avg reward:  0.01\n",
      "episode:  2500 avg reward:  0.0\n",
      "episode:  3000 avg reward:  0.02\n",
      "episode:  3500 avg reward:  0.02\n",
      "episode:  4000 avg reward:  0.03\n",
      "episode:  4500 avg reward:  0.0\n",
      "episode:  5000 avg reward:  0.01\n",
      "episode:  5500 avg reward:  0.03\n",
      "episode:  6000 avg reward:  0.02\n",
      "episode:  6500 avg reward:  0.02\n",
      "episode:  7000 avg reward:  0.02\n",
      "episode:  7500 avg reward:  0.01\n",
      "episode:  8000 avg reward:  0.01\n",
      "episode:  8500 avg reward:  0.0\n",
      "episode:  9000 avg reward:  0.03\n",
      "episode:  9500 avg reward:  0.02\n",
      "episode:  10000 avg reward:  0.03\n",
      "episode:  10500 avg reward:  0.06\n",
      "episode:  11000 avg reward:  0.04\n",
      "episode:  11500 avg reward:  0.04\n",
      "episode:  12000 avg reward:  0.04\n",
      "episode:  12500 avg reward:  0.02\n",
      "episode:  13000 avg reward:  0.05\n",
      "episode:  13500 avg reward:  0.02\n",
      "episode:  14000 avg reward:  0.02\n",
      "episode:  14500 avg reward:  0.03\n",
      "episode:  15000 avg reward:  0.04\n",
      "episode:  15500 avg reward:  0.08\n",
      "episode:  16000 avg reward:  0.09\n",
      "episode:  16500 avg reward:  0.05\n",
      "episode:  17000 avg reward:  0.09\n",
      "episode:  17500 avg reward:  0.09\n",
      "episode:  18000 avg reward:  0.09\n",
      "episode:  18500 avg reward:  0.12\n",
      "episode:  19000 avg reward:  0.09\n",
      "episode:  19500 avg reward:  0.11\n",
      "episode:  20000 avg reward:  0.19\n",
      "episode:  20500 avg reward:  0.15\n",
      "episode:  21000 avg reward:  0.12\n",
      "episode:  21500 avg reward:  0.08\n",
      "episode:  22000 avg reward:  0.11\n",
      "episode:  22500 avg reward:  0.16\n",
      "episode:  23000 avg reward:  0.23\n",
      "episode:  23500 avg reward:  0.26\n",
      "episode:  24000 avg reward:  0.11\n",
      "episode:  24500 avg reward:  0.18\n",
      "episode:  25000 avg reward:  0.16\n",
      "episode:  25500 avg reward:  0.19\n",
      "episode:  26000 avg reward:  0.14\n",
      "episode:  26500 avg reward:  0.27\n",
      "episode:  27000 avg reward:  0.22\n",
      "episode:  27500 avg reward:  0.4\n",
      "episode:  28000 avg reward:  0.56\n",
      "episode:  28500 avg reward:  0.64\n",
      "episode:  29000 avg reward:  0.63\n",
      "episode:  29500 avg reward:  0.6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "np.random.seed(3)\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "#env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "num_games = 30000\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "learning_rate = 0.1\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=learning_rate))\n",
    "    return model\n",
    "\n",
    "def softmax(x):\n",
    "    return \n",
    "\n",
    "model = create_model()\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    s = []\n",
    "    a = []\n",
    "\n",
    "    while(not done): \n",
    "        action_prob = model.predict(state)[0]\n",
    "        action = np.random.choice(action_space, p=action_prob)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        \n",
    "        s.append(state[0])\n",
    "        a.append(np.identity(action_space)[action:action+1][0])\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            reward_list.append(episode_reward)\n",
    "            \n",
    "            R = episode_reward - np.mean(reward_list)\n",
    "            \n",
    "            y = np.array(a)*R\n",
    "            model.train_on_batch(np.array(s), y)\n",
    "            \n",
    "            if(game%500 == 0):\n",
    "                print('episode: ', game, 'avg reward: ', np.mean(reward_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now lets try to get to something similar with some math to back it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to derive backprop in supervised learning we first need a cost function. Since we don't want to minimize the cost but instead increase the reward this is called score function.\n",
    "\n",
    "## Policy score function J($\\theta$) / also called advantage function\n",
    "The policy score function calculates the expected reward of a policy. Three methos work equally well depending on the environment.\n",
    "### 1. Episodic environments: Using the mean from start to end\n",
    "Calculate the mean return from start to the end of the episode <br>\n",
    "$J_1(\\theta) = E_\\pi [R_1 + \\gamma R_2 + \\gamma^2 R_3 + ...] = E_\\pi(v(s_1)) = V^{\\pi_\\theta}(s_1)$\n",
    "\n",
    "### 2. Continous environments: Using the average value\n",
    "In continous environments we can not rely on a specific start state. Instead, the states are weighted based on how often they are visited and this weight is multiplied with the expected reward from this state onwards. <br>\n",
    "$J_{avg}(\\theta) = E_\\pi(V(s)) = \\sum_s d(s)V(s)$ with $d(s) = N(s)/(\\sum_{s'} N(s'))$\n",
    "\n",
    "### 3. Continous environments: Using the average reward per time step\n",
    "sum over probability to be in state s <br>\n",
    "multiplied by <br>\n",
    "sum over probability to take action a from that state unter that policy <br>\n",
    "multiplied by <br>\n",
    "Reward for that action in that state <br>\n",
    "$J_{avR}(\\theta) = E_\\pi(r) = \\sum_s d(s) \\sum_a \\pi_\\theta (a|s) R^a_s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient ascent\n",
    "Instead of gradient descent for a loss function like in supervised learning, we use gradient ascent for the score function. <br>\n",
    "Update: $\\theta$ <-- $ \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla_\\pi J(\\theta) = \\nabla_\\theta \\sum_t \\pi(a|s, \\theta)R(t)$ <br>\n",
    "$ \\nabla_\\pi J(\\theta) =  \\sum_t \\nabla_\\theta \\pi(a|s, \\theta)R(t)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Likelihood ratio trick  <br>\n",
    "\\begin{equation*}\n",
    "\\nabla log(x) =  \\frac{\\nabla x}{x}\n",
    "\\end{equation*}\n",
    "We divide and multiply by $\\pi(a | s, \\theta)$, which results in: <br>\n",
    "\\begin{equation*}\n",
    "\\nabla_\\theta \\pi(a | s, \\theta) = \\pi(a | s, \\theta)  \\frac{ \\nabla_\\theta \\pi(a | s, \\theta) }{ \\pi(a | s, \\theta) }\n",
    "\\end{equation*}\n",
    "\n",
    "$ \\nabla_\\pi J(\\theta) =  \\sum_t \\nabla_\\theta \\pi(t, \\theta)R(t) = \\sum_t  \\pi(t, \\theta) \\nabla_\\theta (log (\\pi(t, \\theta)))R(t) $ <br>\n",
    " <br>\n",
    "Since $\\sum_t  \\pi(t, \\theta)$ is a sum over the probabilies we can convert the sum to an expectation <br>\n",
    "$ \\nabla_\\pi J(\\theta) =  E_\\pi [ \\nabla_\\theta (log (\\pi(t, \\theta)))R(t) ] $ <br>\n",
    "<br>\n",
    "As we have seen in the monte-carlo approach, expectations can be approximated by m empirical sample episodes. <br>\n",
    "$ \\nabla_\\pi J(\\theta) =  1/m \\sum_{i=1}^m \\nabla_\\theta (log (\\pi(t_i, \\theta)))R(t_i)  $ <br>\n",
    "<br>\n",
    " \n",
    "We can change the update rule: <br>\n",
    "$\\theta$ <-- $\\theta + \\alpha \\nabla_\\theta(log(\\pi(a|s,\\theta)))R(t)$ <br>\n",
    "\n",
    "Keras can calculate the gradients for us and update the network but the loss function has to be implemented using Keras-Backend. \n",
    "For the rewards two tricks can be used to speed up the computation. Subtract the mean of the past rewards from the reward and devide it by the standard deviation of past rewards. Deviding by the standard deviation scales the gradients and subtracting the mean lets the agent take steps into the oposite direction (cases where the agent ) had a worse performance than usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game:  10 avg reward:  20.0\n",
      "game:  20 avg reward:  19.5\n",
      "game:  30 avg reward:  19.0\n",
      "game:  40 avg reward:  25.3\n",
      "game:  50 avg reward:  30.22\n",
      "game:  60 avg reward:  34.2\n",
      "game:  70 avg reward:  37.17142857142857\n",
      "game:  80 avg reward:  40.7625\n",
      "game:  90 avg reward:  43.44444444444444\n",
      "game:  100 avg reward:  44.77\n",
      "game:  110 avg reward:  51.49\n",
      "game:  120 avg reward:  58.11\n",
      "game:  130 avg reward:  66.63\n",
      "game:  140 avg reward:  74.61\n",
      "game:  150 avg reward:  81.39\n",
      "game:  160 avg reward:  93.38\n",
      "game:  170 avg reward:  106.45\n",
      "game:  180 avg reward:  118.73\n",
      "game:  190 avg reward:  131.79\n",
      "game:  200 avg reward:  146.12\n",
      "game:  210 avg reward:  156.77\n",
      "game:  220 avg reward:  167.3\n",
      "game:  230 avg reward:  176.9\n",
      "game:  240 avg reward:  184.37\n",
      "game:  250 avg reward:  190.8\n",
      "game:  260 avg reward:  192.97\n",
      "game:  270 avg reward:  193.25\n",
      "game:  280 avg reward:  194.38\n",
      "game:  290 avg reward:  194.83\n",
      "game:  300 avg reward:  194.15\n"
     ]
    }
   ],
   "source": [
    "# seed=1, 2, 3 functions nicely\n",
    "# seed=4 initializes the neural net such that the network gets unstable, \n",
    "# outputs a probability of zero to take a certain action and the log(0) is not defiend. \n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import gym\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import keras.backend as K\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "batch_size = 10\n",
    "gamma = 0.99\n",
    "num_games = 300\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "losses = []\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "def discount_rewards(r, gamma=gamma):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])\n",
    "\n",
    "\n",
    "def create_model(env):\n",
    "    num_actions = env.reset().shape    \n",
    "    inp = Input(shape=[state_space], name='input_x')\n",
    "    adv = Input(shape=[1], name='advantage')\n",
    "    \n",
    "    hidden1 = Dense(8, input_dim=state_space, activation='relu', \n",
    "                    kernel_initializer='random_uniform', use_bias=False)(inp)\n",
    "    outp = Dense(action_space, activation='softmax', \n",
    "                 kernel_initializer='random_uniform', use_bias=False)(hidden1)\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        log_prob = -K.log(y_pred)\n",
    "        return K.sum(log_prob*adv, keepdims=True)\n",
    "\n",
    "\n",
    "    model_train = Model(inputs=[inp, adv], outputs=outp)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr=learning_rate))\n",
    "    model_predict = Model(inputs=[inp], outputs=outp)\n",
    "    return model_train, model_predict\n",
    "\n",
    "\n",
    "model_train, model_predict = create_model(env)\n",
    "s = np.empty(0).reshape(0, state_space)\n",
    "a = np.empty(0).reshape(0,1)\n",
    "r = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "np.seterr(all='raise') # throws an exception if log(0) in the loss function messes up the gradient\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_space])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while(not done): \n",
    "        action_prob = model_predict.predict(state)\n",
    "        action = np.random.choice(action_space, p=action_prob[0])\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.reshape(state_next, [1, state_space])\n",
    "        s = np.vstack([s, state])\n",
    "        r = np.vstack([r, reward])\n",
    "        a = np.vstack([a, action])\n",
    "        episode_reward += reward\n",
    "        state = state_next\n",
    "        \n",
    "        if(done):\n",
    "            discounted_rewards_episode = discount_rewards(r, gamma=gamma)\n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "            reward_list.append(episode_reward)\n",
    "            r = np.empty(0).reshape(0,1)\n",
    "            \n",
    "            if (game+1)%10 == 0:\n",
    "                print('game: ', game+1, 'avg reward: ', np.mean(reward_list))\n",
    "                \n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= (discounted_rewards.std() + 1e-5)\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            a = a.squeeze().astype(int)\n",
    "\n",
    "            actions_train = np.zeros([len(a), action_space])\n",
    "            actions_train[np.arange(len(a)), a] = 1\n",
    "            \n",
    "            loss = model_train.train_on_batch([s, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Clear out game variables\n",
    "            s = np.empty(0).reshape(0,state_space)\n",
    "            a = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
