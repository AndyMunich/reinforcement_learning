{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 09. Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value based methods like Q-learning extract the policy out of the value function. But what do you do if the action space is to huge or has continous values? Policy Gradient Methods just like genetic algorithms work directly on the policy and don't take the detour over the value function. <br>\n",
    "<br>\n",
    "__Advantages of policy gradient methods:__ <br>\n",
    "- value based methods can have huge oscilattions while training since very small changes in Q can result in other policies, following the gradient on the other hand should result in smooth policy updates\n",
    "- Policy gradients can handle high dimensional action spaces\n",
    "- Policy gradients can learn stochastic policies which is advantageous in games like stone paper scissors\n",
    "<br>\n",
    "<br>\n",
    "__Disadvantages:__ <br>\n",
    "- getting stuck in local optima\n",
    "- can take more iterations for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning you get labels with each dataset. e.g. images of cats and dogs and the job is to classify them. Lets say the network outputs a probability of [0.6, 0.4] while the label was [0, 1], the standard approach would be to subtract the prediction from the label / feed it in a loss function and backpropagate the error such that the weights get shifted rowards better predictions.\n",
    "\n",
    "Policy gradients are very similar to this approach. Instead of a given Label we got some reward. If we recieved high rewards during the episode, all actions we took are seen as correct decisions.\n",
    "In the case of low rewards we want to change the weights of our policy such that the bad decisions are taken less likely.\n",
    "\n",
    "A simple criterium to seperate good from bad decisions could be to subtract the average reward from our current one. A positive number means we got more than usual, a negative number means the agent performed worse, and the value can be used as a weight, how much we want to change the policy. Lets call this number advantage (adv)\n",
    "\n",
    "For every step of the episode our policy gives us the action probablity:\n",
    "state --> policy --> probability to take an action (prob_a) e.g. [0.6, 0.4] \n",
    "\n",
    "Lets say we got a reward which was better than average and the taken action was [1.0, 0], than we can use this sample just like in supervised learning for backpropagation with model.train_on_batch(state, [1.0*adv, 0]) to change the parameter such that the output gets closer to our desired output.\n",
    "\n",
    "In the case of a low reward (less than average), we want to lower the probability for the chosen action. This is implemented using model.train_on_batch(state, [1.0*adv, 0]) where adv is a negative number.\n",
    "\n",
    "Lets see if this simple approach already works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 avg reward:  0.0\n",
      "episode:  500 avg reward:  0.0\n",
      "episode:  1000 avg reward:  0.0\n",
      "episode:  1500 avg reward:  0.03\n",
      "episode:  2000 avg reward:  0.0\n",
      "episode:  2500 avg reward:  0.01\n",
      "episode:  3000 avg reward:  0.0\n",
      "episode:  3500 avg reward:  0.04\n",
      "episode:  4000 avg reward:  0.01\n",
      "episode:  4500 avg reward:  0.04\n",
      "episode:  5000 avg reward:  0.03\n",
      "episode:  5500 avg reward:  0.0\n",
      "episode:  6000 avg reward:  0.03\n",
      "episode:  6500 avg reward:  0.01\n",
      "episode:  7000 avg reward:  0.04\n",
      "episode:  7500 avg reward:  0.02\n",
      "episode:  8000 avg reward:  0.0\n",
      "episode:  8500 avg reward:  0.04\n",
      "episode:  9000 avg reward:  0.01\n",
      "episode:  9500 avg reward:  0.02\n",
      "episode:  10000 avg reward:  0.01\n",
      "episode:  10500 avg reward:  0.0\n",
      "episode:  11000 avg reward:  0.04\n",
      "episode:  11500 avg reward:  0.01\n",
      "episode:  12000 avg reward:  0.0\n",
      "episode:  12500 avg reward:  0.05\n",
      "episode:  13000 avg reward:  0.01\n",
      "episode:  13500 avg reward:  0.02\n",
      "episode:  14000 avg reward:  0.03\n",
      "episode:  14500 avg reward:  0.02\n",
      "episode:  15000 avg reward:  0.05\n",
      "episode:  15500 avg reward:  0.04\n",
      "episode:  16000 avg reward:  0.0\n",
      "episode:  16500 avg reward:  0.05\n",
      "episode:  17000 avg reward:  0.08\n",
      "episode:  17500 avg reward:  0.05\n",
      "episode:  18000 avg reward:  0.06\n",
      "episode:  18500 avg reward:  0.06\n",
      "episode:  19000 avg reward:  0.09\n",
      "episode:  19500 avg reward:  0.16\n",
      "episode:  20000 avg reward:  0.14\n",
      "episode:  20500 avg reward:  0.3\n",
      "episode:  21000 avg reward:  0.31\n",
      "episode:  21500 avg reward:  0.42\n",
      "episode:  22000 avg reward:  0.51\n",
      "episode:  22500 avg reward:  0.58\n",
      "episode:  23000 avg reward:  0.58\n",
      "episode:  23500 avg reward:  0.56\n",
      "episode:  24000 avg reward:  0.59\n",
      "episode:  24500 avg reward:  0.55\n",
      "episode:  25000 avg reward:  0.69\n",
      "episode:  25500 avg reward:  0.74\n",
      "episode:  26000 avg reward:  0.69\n",
      "episode:  26500 avg reward:  0.68\n",
      "episode:  27000 avg reward:  0.71\n",
      "episode:  27500 avg reward:  0.71\n",
      "episode:  28000 avg reward:  0.71\n",
      "episode:  28500 avg reward:  0.73\n",
      "episode:  29000 avg reward:  0.69\n",
      "episode:  29500 avg reward:  0.69\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "from keras.initializers import RandomUniform\n",
    "np.random.seed(42)\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "#env_name = 'Taxi-v2'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "gamma = 0.999\n",
    "num_games = 30000\n",
    "\n",
    "reward_list = deque(maxlen=100)\n",
    "learning_rate = 0.1\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=state_space, activation='relu'))\n",
    "    model.add(Dense(action_space, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=learning_rate))\n",
    "    return model\n",
    "\n",
    "def softmax(x):\n",
    "    return \n",
    "\n",
    "model = create_model()\n",
    "for game in range(num_games):\n",
    "    state = env.reset()\n",
    "    state = np.identity(state_space)[state:state+1] # transforms state into 1-hot-encoding\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    s = []\n",
    "    a = []\n",
    "\n",
    "    while(not done): \n",
    "        action_prob = model.predict(state)[0]\n",
    "        action = np.random.choice(action_space, p=action_prob)\n",
    "\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.identity(state_space)[state_next:state_next+1]\n",
    "        \n",
    "        s.append(state[0])\n",
    "        a.append(np.identity(action_space)[action:action+1][0])\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        state = state_next\n",
    "        if(done):\n",
    "            reward_list.append(episode_reward)\n",
    "            \n",
    "            R = episode_reward - np.mean(reward_list)\n",
    "            \n",
    "            y = np.array(a)*R\n",
    "            model.train_on_batch(np.array(s), y)\n",
    "            \n",
    "            if(game%500 == 0):\n",
    "                print('episode: ', game, 'avg reward: ', np.mean(reward_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now lets try to get to something similar with some math to back it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to derive backprop in supervised learning we fest need a cost function. Since we don't want to minimize the cost but instead increase the reward this is called score function.\n",
    "\n",
    "## Policy score function J($\\theta$) / also called advantage function\n",
    "The policy score function calculates the expected reward of a policy. Three methos work equally well depending on the environment.\n",
    "### 1. Episodic environments: Using the mean from start to end\n",
    "Calculate the mean return from start to the end of the episode <br>\n",
    "$J_1(\\theta) = E_\\pi [R_1 + \\gamma R_2 + \\gamma^2 R_3 + ...] = E_\\pi(v(s_1)) = V^{\\pi_\\theta}(s_1)$\n",
    "\n",
    "### 2. Continous environments: Using the average value\n",
    "In continous environments we can not rely on a specific start state. Instead, the states are weighted based on how often they are visited and this weight is multiplied with the expected reward from this state onwards. <br>\n",
    "$J_{avg}(\\theta) = E_\\pi(V(s)) = \\sum_s d(s)V(s)$ with $d(s) = N(s)/(\\sum_{s'} N(s'))$\n",
    "\n",
    "### 3. Continous environments: Using the average reward per time step\n",
    "sum over probability to be in state s <br>\n",
    "multiplied by <br>\n",
    "sum over probability to take action a from that state unter that policy <br>\n",
    "multiplied by <br>\n",
    "Reward for that action in that state <br>\n",
    "$J_{avR}(\\theta) = E_\\pi(r) = \\sum_s d(s) \\sum_a \\pi_\\theta (s,a) R^a_s$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient ascent\n",
    "Instead of gradient descent for a loss function like in supervised learning, we use gradient ascent for the score function. <br>\n",
    "Update: $\\theta$ <-- $ \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla_\\pi J(\\theta) = \\nabla_\\theta \\sum_t \\pi(t, \\theta)R(t)$ <br>\n",
    "$ \\nabla_\\pi J(\\theta) =  \\sum_t \\nabla_\\theta \\pi(t, \\theta)R(t)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Likelihood ratio trick  <br>\n",
    "\\begin{equation*}\n",
    "\\nabla log(x) =  \\frac{\\nabla x}{x}\n",
    "\\end{equation*}\n",
    "We divide and multiply by $\\pi(t, \\theta)$, which results in: <br>\n",
    "\\begin{equation*}\n",
    "\\nabla_\\theta \\pi(t, \\theta) = \\pi(t, \\theta)  \\frac{ \\nabla_\\theta \\pi(t, \\theta) }{ \\pi(t, \\theta) }\n",
    "\\end{equation*}\n",
    "\n",
    "$ \\nabla_\\pi J(\\theta) =  \\sum_t \\nabla_\\theta \\pi(t, \\theta)R(t) = \\sum_t  \\pi(t, \\theta) \\nabla_\\theta (log (\\pi(t, \\theta)))R(t) $ <br>\n",
    " <br>\n",
    "Since $\\sum_t  \\pi(t, \\theta)$ is a sum over the probabilies we can convert the sum to an expectation <br>\n",
    "$ \\nabla_\\pi J(\\theta) =  E_\\pi [ \\nabla_\\theta (log (\\pi(t, \\theta)))R(t) ] $ <br>\n",
    "<br>\n",
    " \n",
    "We can change the update rule: <br>\n",
    "$\\theta$ <-- $\\theta + \\alpha \\nabla_\\theta(log(\\pi(s,a,\\theta)))R(t)$ <br>\n",
    "<br>\n",
    "\n",
    "Keras can calculate the gradients for us and update the network but the loss function has to be implemented using Keras-Backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong state shape is given: ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-21baeb6af8d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-21baeb6af8d0>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-21baeb6af8d0>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong state shape is given: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong state shape is given: ()"
     ]
    }
   ],
   "source": [
    "# source: https://gist.githubusercontent.com/kkweon/c8d1caabaf7b43317bc8825c226045d2/raw/fb433ae27c57aa41883af613d88227c65e8fb5ab/policy_gradient.py\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[10]):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.__build_network(input_dim, output_dim, hidden_dims)\n",
    "        self.__build_train_fn()\n",
    "\n",
    "    def __build_network(self, input_dim, output_dim, hidden_dims=[10]):\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "\n",
    "        net = layers.Dense(output_dim)(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        action_prob_placeholder = self.model.output\n",
    "        action_onehot_placeholder = K.placeholder(shape=(None, self.output_dim),\n",
    "                                                  name=\"action_onehot\")\n",
    "        discount_reward_placeholder = K.placeholder(shape=(None,),\n",
    "                                                    name=\"discount_reward\")\n",
    "\n",
    "        action_prob = K.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
    "        log_action_prob = K.log(action_prob)\n",
    "\n",
    "        loss = - log_action_prob * discount_reward_placeholder\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "        adam = optimizers.Adam()\n",
    "\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,\n",
    "                                   loss=loss)\n",
    "\n",
    "        self.train_fn = K.function(inputs=[self.model.input,\n",
    "                                           action_onehot_placeholder,\n",
    "                                           discount_reward_placeholder],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        shape = state.shape\n",
    "\n",
    "        if len(shape) == 1:\n",
    "            assert shape == (self.input_dim,), \"{} != {}\".format(shape, self.input_dim)\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        elif len(shape) == 2:\n",
    "            assert shape[1] == (self.input_dim), \"{} != {}\".format(shape, self.input_dim)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Wrong state shape is given: {}\".format(state.shape))\n",
    "\n",
    "        action_prob = np.squeeze(self.model.predict(state))\n",
    "        assert len(action_prob) == self.output_dim, \"{} != {}\".format(len(action_prob), self.output_dim)\n",
    "        return np.random.choice(np.arange(self.output_dim), p=action_prob)\n",
    "\n",
    "    def fit(self, S, A, R):\n",
    "        action_onehot = np_utils.to_categorical(A, num_classes=self.output_dim)\n",
    "        discount_reward = compute_discounted_R(R)\n",
    "\n",
    "        assert S.shape[1] == self.input_dim, \"{} != {}\".format(S.shape[1], self.input_dim)\n",
    "        assert action_onehot.shape[0] == S.shape[0], \"{} != {}\".format(action_onehot.shape[0], S.shape[0])\n",
    "        assert action_onehot.shape[1] == self.output_dim, \"{} != {}\".format(action_onehot.shape[1], self.output_dim)\n",
    "        assert len(discount_reward.shape) == 1, \"{} != 1\".format(len(discount_reward.shape))\n",
    "\n",
    "        self.train_fn([S, action_onehot, discount_reward])\n",
    "\n",
    "\n",
    "def compute_discounted_R(R, discount_rate=.99):\n",
    "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(R))):\n",
    "\n",
    "        running_add = running_add * discount_rate + R[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    discounted_r -= discounted_r.mean() / discounted_r.std()\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def run_episode(env, agent):\n",
    "    done = False\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        s2, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "        if done:\n",
    "            S = np.array(S)\n",
    "            A = np.array(A)\n",
    "            R = np.array(R)\n",
    "\n",
    "            agent.fit(S, A, R)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "\n",
    "try:\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.n\n",
    "    output_dim = env.action_space.n\n",
    "    agent = Agent(input_dim, output_dim, [10])\n",
    "\n",
    "    for episode in range(2000):\n",
    "        reward = run_episode(env, agent)\n",
    "        print(episode, reward)\n",
    "\n",
    "finally:\n",
    "    env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
