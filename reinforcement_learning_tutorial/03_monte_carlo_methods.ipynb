{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Methods have the benefit of not assuming a given model. Instead they estimate the value function by sample returns (just starting from a state a few times, running the policy and calculating how much reward the agent got on average). Depending on the size of the state space, these methods require quite some time to get valid estimates for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "policy = np.random.randint(4, size=state_space)\n",
    "discount_factor=1.0\n",
    "num_episodes = 500\n",
    "v = np.zeros(state_space)\n",
    "q = np.zeros((state_space, action_space))\n",
    "epsilon=1.0\n",
    "\n",
    "returns_sum_v = defaultdict(float)\n",
    "returns_count_v = defaultdict(float)\n",
    "returns_sum_q = defaultdict(float)\n",
    "returns_count_q = defaultdict(float)\n",
    "\n",
    "def mc_policy_evaluation(policy, env, num_episodes, v, q, discount_factor, epsilon):\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        episode = np.empty([0,3])\n",
    "        state = env.reset()\n",
    "        # run an episode and save (state, action, reward) in episode\n",
    "        for t in range(100):\n",
    "            # epsilon greedy\n",
    "            if(np.random.uniform() < epsilon):\n",
    "                action = np.random.choice(4) # exploration\n",
    "            else:\n",
    "                action = policy[state] # exploitation\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode = np.append(episode, np.array([state, action, reward]).reshape(1,3), axis=0)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        states_in_episode = episode[:,0]\n",
    "        first_visit_index = np.unique(states_in_episode, return_index=True)[1]\n",
    "        rewards = episode[:,2]\n",
    "        actions = episode[:,1]\n",
    "        \n",
    "        # calculates the reward for each state from the point it was first visited\n",
    "        for x in first_visit_index:\n",
    "            state = int(states_in_episode[x])\n",
    "            action = int(actions[x])\n",
    "            G = sum([r*(discount_factor**i) for i,r in enumerate(rewards[x:])])\n",
    "            returns_sum_v[state] += G\n",
    "            returns_count_v[state] += 1.0\n",
    "            v[state] = returns_sum_v[state] / returns_count_v[state]\n",
    "            \n",
    "            returns_sum_q[state, action] += G\n",
    "            returns_count_q[state, action] += 1.0\n",
    "            q[state, action] = returns_sum_q[state, action] / returns_count_q[state, action]\n",
    "\n",
    "    return v, q\n",
    "\n",
    "v, q = mc_policy_evaluation(policy, env, num_episodes, v, q, discount_factor, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control\n",
    "Like policy iteration the agent rotates between improving the estimate of the value function and improving the policy by acting greedy on the improved value function. Using the state value function has one problem, it requires a model of the environment and Monte Carlo tries to be model free. $\\pi'(s) = argmax( R_s^a + P_{s  s'}^a v(s') )$. <br>\n",
    "The alternative is to use Q, which does not require an environment model. $\\pi'(s) = argmax( Q(s,a) )$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount_factor=1.0\n",
    "num_episodes = 100\n",
    "q = np.zeros((state_space, action_space))\n",
    "v = np.zeros(state_space)\n",
    "policy = np.random.randint(4, size=state_space)\n",
    "epsilon = 1.0\n",
    "\n",
    "policy = np.random.randint(4, size=state_space)\n",
    "discount_factor=1.0\n",
    "num_episodes = 500\n",
    "\n",
    "for i in range(1000):\n",
    "    v, q = mc_policy_evaluation(policy, env, num_episodes, v, q, discount_factor, epsilon)\n",
    "    policy = np.argmax(q, axis=1)\n",
    "    epsilon *= 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the state value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4011143  0.21144849 0.23839718 0.09157032]\n",
      " [0.42370662 0.         0.26209566 0.        ]\n",
      " [0.48507557 0.5635991  0.54627573 0.        ]\n",
      " [0.         0.68083442 0.8302095  0.        ]]\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def render_value_fct_array(value_fct_array):\n",
    "    len_x = int(np.sqrt(value_fct_array.shape[0]))\n",
    "    array_2d = value_fct_array.reshape([ len_x, len_x])\n",
    "    print(array_2d)\n",
    "\n",
    "np.set_printoptions(linewidth = 150)\n",
    "render_value_fct_array(v)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the policy in 1000 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722\n"
     ]
    }
   ],
   "source": [
    "def test_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    for i in range(100):\n",
    "        action = policy[state]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        if(done):\n",
    "            break\n",
    "    return reward_sum\n",
    "\n",
    "reward_all = 0\n",
    "for i in range(1000):\n",
    "    reward = test_policy(env, policy)\n",
    "    reward_all += reward\n",
    "\n",
    "print(reward_all/1000.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
