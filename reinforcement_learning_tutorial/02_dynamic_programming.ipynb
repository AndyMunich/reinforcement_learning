{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming refers to methods which calculate the optimal policy given a perfect model of the environment as a Markov Decision process.\n",
    "In real world applications a perfect model is usually not given and the amount of states yields great computational expense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming is based on the bellman equation for the value function. <br>\n",
    "**state value function** <br>\n",
    "$v_\\pi(s) = \\sum_a \\pi(s,a) \\sum_{s'} P^a_{s s'}[R^a_{s s'} + \\gamma v^\\pi (s')] $\n",
    "\n",
    "**action value function** <br>\n",
    "$q_\\pi(s) =  \\sum_{s'} P^a_{s s'}[R^a_{s s'} + \\gamma \\sum_{a'}\\pi(s',a')Q^\\pi(s',a') ] $\n",
    "\n",
    "This formula only says that the value of the current state $v_\\pi(s)$ has to be equal to the value of all possible next states $s'$, weighted by the probability to get to that state + the reward to get on the way there. This probability depends on your policy (which action you choose in that state) $\\pi(s,a)$ and the transition probability (how the environment reacts depending on your action) $P^a_{s s'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "Starting with value iteration (right side of the image below), the bellman equation is applied to every state until the values of all states converge. After the value function for every state is known, the policy is extracted by acting greedy in every state/picking the next state with the highest value function.\n",
    "The intuitive way to understand why it works, is that there has to be some goal state which the agent has to reach, for this state the value function already has the right value (e.g. +1 for a won game).\n",
    "After applying the bellman equation the values around this goal state get shifted into the right direction. \n",
    "A few iterations later the values around the goal state get closer and closer to the correct value and those correct values get back-propagated to the next states, following this approach the right values back-propagate over the entire state space until they all converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/value_policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample implementation for Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82058182 0.81961477 0.81894265 0.81860067]\n",
      " [0.82087832 0.         0.52740189 0.        ]\n",
      " [0.82134266 0.82193808 0.76331139 0.        ]\n",
      " [0.         0.88124695 0.94061371 0.        ]]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "v = np.zeros(state_space)\n",
    "q = np.zeros([state_space, action_space])\n",
    "gamma = 1.0\n",
    "epsilon = 0.001\n",
    "\n",
    "while(True):\n",
    "    v_backup = v.copy()\n",
    "    for s in range(state_space):\n",
    "        for a in range(action_space):\n",
    "            env_P = np.array(env.env.P[s][a])\n",
    "            trans_prob = env_P[:,0]\n",
    "            rewards = env_P[:,2]\n",
    "            s_prime = env_P[:,1].astype(int)\n",
    "            exp_r = np.dot(np.transpose(trans_prob), rewards)\n",
    "            q[s,a] = exp_r +  gamma * np.dot(np.transpose(trans_prob), v[s_prime])\n",
    "        v[s] = np.amax(q[s,:])\n",
    "        \n",
    "    if(np.sum(np.abs(v - v_backup)) < epsilon): # convergence criterium\n",
    "        break\n",
    "\n",
    "def render_value_fct_array(value_fct_array):\n",
    "    len_x = int(np.sqrt(value_fct_array.shape[0]))\n",
    "    array_2d = value_fct_array.reshape([ len_x, len_x])\n",
    "    print(array_2d)\n",
    "\n",
    "np.set_printoptions(linewidth = 150)\n",
    "render_value_fct_array(v)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of value iteration is that the right policy might have already been given before the values converge and the while loop could have been stopped earlier. This is why policy iteration which stops after the policy does not change after an iteration, is faster.\n",
    "It took me a while to understand that, since the policy evaluation part is nearly the same as the entire value iteration, and it is repeated multiple times I thought it must be slower.\n",
    "The key is that the values for each state get only initialized once so if the policy evaluation runs the second time, the policy has only changed a bit and it needs considerably less iterations to converge again which applies to all following repetitions of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample implementation for policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54202593 0.49880319 0.47069569 0.4568517 ]\n",
      " [0.55845096 0.         0.35834807 0.        ]\n",
      " [0.59179874 0.64307982 0.61520756 0.        ]\n",
      " [0.         0.74172044 0.86283743 0.        ]]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get the value function\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "v = np.zeros(state_space)\n",
    "q = np.zeros([state_space, action_space])\n",
    "gamma = 0.99\n",
    "epsilon = 1e-10\n",
    "policy = np.random.randint(4, size=[state_space])\n",
    "\n",
    "def policy_evaluation(env, policy, v):\n",
    "    while(True):\n",
    "        v_prev = v.copy()\n",
    "        for s in range(state_space):\n",
    "            temp = v.copy()\n",
    "            a = policy[s]\n",
    "            env_P = np.array(env.env.P[s][a])\n",
    "            trans_prob = env_P[:,0]\n",
    "            rewards = env_P[:,2]\n",
    "            s_prime = env_P[:,1].astype(int)\n",
    "            v[s] = np.sum( trans_prob * (rewards + gamma*v[s_prime]))\n",
    "        if(np.sum(np.abs( v_prev - v )) < epsilon):\n",
    "            return v\n",
    "    \n",
    "def policy_improvement(env, policy, v):\n",
    "    for s in range(state_space):\n",
    "        v_prime = np.zeros(action_space)\n",
    "        for a in range(action_space):\n",
    "            env_P = np.array(env.env.P[s][a])\n",
    "            trans_prob = env_P[:,0]\n",
    "            rewards = env_P[:,2]\n",
    "            s_prime = env_P[:,1].astype(int)\n",
    "            v_prime[a] = np.sum( trans_prob * (rewards + gamma*v[s_prime]) )\n",
    "        policy[s] = np.argmax(v_prime)        \n",
    "    \n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, policy, v):\n",
    "    max_iter = 1000000\n",
    "    for i in range(max_iter):\n",
    "        old_policy = policy.copy()\n",
    "        v = policy_evaluation(env, policy, v)\n",
    "        policy = policy_improvement(env, policy, v)\n",
    "        finished = np.all(old_policy == policy)\n",
    "        \n",
    "        if(finished):\n",
    "            return policy, v\n",
    "\n",
    "policy, v = policy_iteration(env, policy, v)\n",
    "render_value_fct_array(v)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
